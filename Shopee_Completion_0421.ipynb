{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 라이브러리 모음","metadata":{}},{"cell_type":"code","source":"!pip install ../input/external-model/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/external-model/efficientnet-1.1.0-py3-none-any.whl\n\n# 파일 처리\nimport os\n\n# Data 처리\nimport pandas as pd\nimport numpy as np\n\n##############################################################\n\n# RAPIDS 라이브러리\nimport cudf, cuml, cupy \nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n##############################################################\n\n# ML, DNN, CNN 관련 라이브러리\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Layer, Input, GlobalAveragePooling2D, Softmax\n\n#!pip install efficientnet\nimport efficientnet.tfkeras as efn\nimport math\n##############################################################\n\n# 이미지 및 그래프 출력\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n##############################################################\n\n# 해쉬(phash) 값 처리\n#import imagehash\n\n##############################################################\n\n# Text Data NLP 처리\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.neighbors import NearestNeighbors\nimport re\nimport nltk\n#nltk.download('popular')\n\nfrom shutil import copyfile\ncopyfile(src = \"../input/you-need-more-tensors-in-neighbourhood/tokenization.py\", dst = \"../working/tokenization.py\")\n\nimport tokenization\nimport tensorflow_hub as hub\n\nfrom sklearn.preprocessing import LabelEncoder\n##############################################################\n\n# 메모리 관리\nimport gc\n\n# 경고메시지 지우기\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n# 상태바 진행상태\nfrom tqdm import tqdm\n\n# Text Color\nfrom termcolor import colored\n\n# 실행시간 확인\nimport time\nimport datetime","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/external-model/Keras_Applications-1.0.8-py3-none-any.whl\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->Keras-Applications==1.0.8) (1.15.0)\nInstalling collected packages: Keras-Applications\nSuccessfully installed Keras-Applications-1.0.8\nProcessing /kaggle/input/external-model/efficientnet-1.1.0-py3-none-any.whl\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (0.18.1)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (1.0.8)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.19.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (2.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.15.0)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.5.4)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.9.0)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.5)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (3.4.0)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.1.1)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (7.2.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2021.3.17)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.8.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (0.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.0) (4.4.2)\nInstalling collected packages: efficientnet\nSuccessfully installed efficientnet-1.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 메모리 관리","metadata":{}},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"We will restrict TensorFlow to max 2GB GPU RAM\nthen RAPIDS can use 14GB GPU RAM\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 변수 모음","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = [512, 512]\n\nBATCH_SIZE = 5\n\nN_CLASSES = 11014\n\nSEED = 42\n\nGET_CV = True\n\nCHECK_SUB =False","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## CV 설정","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 함수 모음","metadata":{}},{"cell_type":"code","source":"# DataSet 불러오기\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].map(lambda x: ' '.join(x))\n        \n        if CHECK_SUB:\n            df = pd.concat([df, df], axis=0)\n            df.reset_index(drop=True, inplace=True)\n        df_cu =cudf.DataFrame(df)\n        image_path = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu =cudf.DataFrame(df)\n        image_path = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_path\n\n# 예측값 결합\ndef combine_preds(row):\n    x = np.concatenate([row['image_pred'], row['text_pred'], row['phash_pred']])\n    return ' '.join(np.unique(x))\n\n\n# F1 Score 함수\ndef f1_score(t_true, t_pred):\n    t_true = t_true.apply(lambda x : set(x.split()))\n    t_pred = t_pred.apply(lambda x : set(x.split()))\n    \n    intersection = np.array([len(x[0] & x[1]) for x in zip(t_true, t_pred)])\n    len_t_true = t_true.apply(lambda x : len(x)).values\n    len_t_pred = t_pred.apply(lambda x : len(x)).values\n    \n    F1 = 2 * intersection / (len_t_true + len_t_pred)\n    \n    return F1\n\n# ArcFace loss 생성 Class\nclass ArcMarginProduct(Layer):\n    '''\n    GDis(Geodestic Distance margin) 구하는 Class\n    Implements large margin arc distance.\n    '''\n    \n    def __init__(self, n_classes, s=30, m=0.5, easy_margin=False, ls_eps=0.0, **kwargs):\n        \n        super(ArcMarginProduct, self).__init__(**kwargs)\n        \n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m =tf.math.cos(m)\n        self.sin_m =tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n        \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'n_classes':self.n_classes,\n            's' : self.s,\n            'm' : self.m,\n            'ls_eps' : self.ls_eps,\n            'easy_margin' : self.easy_margin\n        })\n        return config\n    \n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n        \n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer = 'glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None\n            )\n        \n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        \n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        \n        if self.ls_eps > 0:\n            one_hot = (1-self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n            \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n# KNN 이웃 구하기\ndef get_neighbors(df, embeddings, KNN=50, image=True):\n    model = NearestNeighbors(n_neighbors=KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    if GET_CV:\n        if image :\n            thresholds = list(np.arange(3.0, 5.0, 0.1))\n        else:\n            thresholds = list(np.arange(15, 35, 1))\n            \n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                indc = indices[k , idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[indc].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            \n            print('Our F1 score for threshold {} is {}'.format(threshold, score))\n            scores.append(score)\n            \n        thresholds_scores = pd.DataFrame({\n            'thresholds':thresholds,\n            'scores': scores\n        })\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        \n        print('Our best score : {} , threshold : {}'.format(best_score, best_threshold))\n        \n        del predictions, scores, indc, idx\n        \n        \n        predictions = []\n        for i in range(embeddings.shape[0]):\n            if image:\n                idx = np.where(distances[i,]<3.6)[0]\n            else:\n                idx = np.where(distances[i,]<20.0)[0]\n                \n            ids = indices[i, idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,]<3.6)[0]\n            else:\n                idx = np.where(distances[k,]<20.0)[0]\n            indc = indices[k, idx]\n            posting_ids =df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n            \n    del model, distances, indices, idx, ids, posting_ids\n    gc.collect()\n    \n    return df, predictions\n\n############################################################\n\n# read & decode image\ndef read_and_decode_img(image):\n    image = tf.io.read_file(image)\n    img = tf.image.decode_jpeg(image, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32) / 255.0\n    return img\n\n# dataset load\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_and_decode_img, \n                          num_parallel_calls = tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\n# image embedding\ndef image_embedding(img_path):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_layer = Input(shape = (*IMG_SIZE, 3))\n    label = Input(shape= ())\n    \n    x = efn.EfficientNetB3(weights = None, include_top=False)(input_layer)\n    x = GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n    \n    output_layer = Softmax(dtype='float32')(x)\n    \n    model = Model(inputs=[input_layer, label], \n                  outputs=[output_layer])\n    model.load_weights('../input/shopeeefficientnetb3512/EfficientNetB3_512_42.h5')\n    print(model.summary())\n    model = Model(inputs = model.input[0], \n                  outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(img_path[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    \n    del model\n    \n    image_embeddings = np.concatenate(embeds)\n    print('Image embeddings shape :', image_embeddings.shape)\n    \n    del embeds\n    gc.collect()\n    return image_embeddings\n\n# BERT encoding 함수\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []   # ==>\n    all_masks = []    # ==>\n    all_segments = [] # ==>\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    gc.collect()\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n# BERT Model 전이학습 함수\ndef get_text_embeddings(df, max_len = 70):\n    embeds = []\n    module_url = \"../input/external-model/bert_en_uncased_L-24_H-1024_A-16_1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    \n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = Input(shape = (), name = 'label')\n    \n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    x = margin([clf_output, label])\n    output = Softmax(dtype='float32')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask, segment_ids, label], \n                  outputs = [output])\n    model.load_weights('../input/bert-baseline/Bert_123.h5')\n    \n    model = Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    \n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        \n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        \n        print('총 {}번 작업 중 {}번째 embedding 시작'.format(int(iterator[-1])+1, int(j)+1))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n        print('{}번째 embedding 끝'.format(int(j)+1))\n        \n    del model\n    \n    text_embeddings = np.concatenate(embeds)\n    print('Our text embeddings shape :', text_embeddings.shape)\n    \n    del embeds\n    gc.collect()\n    \n    return text_embeddings\n","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## DataSet 설정","metadata":{}},{"cell_type":"code","source":"df, df_cu, img_paths = read_dataset()\ndf.head()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                             matches  \n0   train_129225211 train_2278313361  \n1  train_3386243561 train_3423213080  \n2  train_2288590299 train_3803689425  \n3  train_2406599165 train_3342059966  \n4   train_3369186413 train_921438619  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>train_3386243561 train_3423213080</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>train_2406599165 train_3342059966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>train_3369186413 train_921438619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Image embeddings","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nimage_embeddings = image_embedding(img_paths)\n\nprint(image_embeddings.shape)\n\ngc.collect()\n\nsec = time.time() - start\ntimes = str(datetime.timedelta(seconds=sec)).split('.')\nprint('총 실행시간 :', times[0])","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n__________________________________________________________________________________________________\nefficientnet-b3 (Functional)    (None, None, None, 1 10783528    input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 1536)         0           efficientnet-b3[0][0]            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None,)]            0                                            \n__________________________________________________________________________________________________\nhead/arc_margin (ArcMarginProdu (None, 11014)        16917504    global_average_pooling2d[0][0]   \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nsoftmax (Softmax)               (None, 11014)        0           head/arc_margin[0][0]            \n==================================================================================================\nTotal params: 27,701,032\nTrainable params: 27,613,736\nNon-trainable params: 87,296\n__________________________________________________________________________________________________\nNone\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\n<ParallelMapDataset shapes: (512, 512, 3), types: tf.float32>\nImage embeddings shape : (34250, 1536)\n(34250, 1536)\n총 실행시간 : 0:06:05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Text embeddings","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\ntext_embeddings = get_text_embeddings(df)\n\nprint(text_embeddings.shape)\n\ngc.collect()\n\nsec = time.time() - start\ntimes = str(datetime.timedelta(seconds=sec)).split('.')\nprint('총 실행시간 :', times[0])","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"총 7번 작업 중 1번째 embedding 시작\n1번째 embedding 끝\n총 7번 작업 중 2번째 embedding 시작\n2번째 embedding 끝\n총 7번 작업 중 3번째 embedding 시작\n3번째 embedding 끝\n총 7번 작업 중 4번째 embedding 시작\n4번째 embedding 끝\n총 7번 작업 중 5번째 embedding 시작\n5번째 embedding 끝\n총 7번 작업 중 6번째 embedding 시작\n6번째 embedding 끝\n총 7번 작업 중 7번째 embedding 시작\n7번째 embedding 끝\nOur text embeddings shape : (34250, 1024)\n(34250, 1024)\n총 실행시간 : 0:06:47\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## image embedding 값으로 prediction data 구하기","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\ndf, image_predictions = get_neighbors(df, image_embeddings, KNN=100, image=True)\n\ngc.collect()\n\nsec = time.time() - start\ntimes = str(datetime.timedelta(seconds=sec)).split('.')\nprint('총 실행시간 :', times[0])\n\ndf.head()","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Our F1 score for threshold 3.0 is 0.7060730652202043\nOur F1 score for threshold 3.1 is 0.7160522989781256\nOur F1 score for threshold 3.2 is 0.7277696148321522\nOur F1 score for threshold 3.3000000000000003 is 0.7397825990142964\nOur F1 score for threshold 3.4000000000000004 is 0.7516964513614298\nOur F1 score for threshold 3.5000000000000004 is 0.7640264855741594\nOur F1 score for threshold 3.6000000000000005 is 0.7772109520816285\nOur F1 score for threshold 3.7000000000000006 is 0.7903100905418591\nOur F1 score for threshold 3.8000000000000007 is 0.8030970222483025\nOur F1 score for threshold 3.900000000000001 is 0.8153789968925708\nOur F1 score for threshold 4.000000000000001 is 0.8257123577977383\nOur F1 score for threshold 4.100000000000001 is 0.833837438596564\nOur F1 score for threshold 4.200000000000001 is 0.83790237211349\nOur F1 score for threshold 4.300000000000001 is 0.8365055765380428\nOur F1 score for threshold 4.400000000000001 is 0.8268638556543608\nOur F1 score for threshold 4.500000000000002 is 0.8056991619902377\nOur F1 score for threshold 4.600000000000001 is 0.7698092189811044\nOur F1 score for threshold 4.700000000000001 is 0.7196655396560101\nOur F1 score for threshold 4.800000000000002 is 0.6588458292334153\nOur F1 score for threshold 4.900000000000002 is 0.5909820873340552\nOur best score : 0.83790237211349 , threshold : 4.200000000000001\n총 실행시간 : 0:00:58\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                             matches  \\\n0   train_129225211 train_2278313361   \n1  train_3386243561 train_3423213080   \n2  train_2288590299 train_3803689425   \n3  train_2406599165 train_3342059966   \n4   train_3369186413 train_921438619   \n\n                                        pred_matches        f1  \n0                                    train_129225211  0.666667  \n1  train_3386243561 train_3423213080 train_183194...  0.222222  \n2                  train_2288590299 train_3803689425  1.000000  \n3  train_2406599165 train_3576714541 train_334205...  0.105263  \n4  train_3369186413 train_921438619 train_2522158...  0.039216  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>matches</th>\n      <th>pred_matches</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>train_129225211 train_2278313361</td>\n      <td>train_129225211</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>train_3386243561 train_3423213080</td>\n      <td>train_3386243561 train_3423213080 train_183194...</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>train_2288590299 train_3803689425</td>\n      <td>train_2288590299 train_3803689425</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>train_2406599165 train_3342059966</td>\n      <td>train_2406599165 train_3576714541 train_334205...</td>\n      <td>0.105263</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>train_3369186413 train_921438619</td>\n      <td>train_3369186413 train_921438619 train_2522158...</td>\n      <td>0.039216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## text embeddings 값으로 prediction data 구하기","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\ndf, text_predictions = get_neighbors(df, text_embeddings, KNN=100, image=False)\n\ngc.collect()\n\nsec = time.time() - start\ntimes = str(datetime.timedelta(seconds=sec)).split('.')\nprint('총 실행시간 :', times[0])\n\ndf.head()","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Our F1 score for threshold 15 is 0.7238646050555128\nOur F1 score for threshold 16 is 0.7521519309645522\nOur F1 score for threshold 17 is 0.7735370949071356\nOur F1 score for threshold 18 is 0.7888929328931714\nOur F1 score for threshold 19 is 0.8005198328263164\nOur F1 score for threshold 20 is 0.8101082361910849\nOur F1 score for threshold 21 is 0.8186452450831521\nOur F1 score for threshold 22 is 0.8258172895366394\nOur F1 score for threshold 23 is 0.8323012301249347\nOur F1 score for threshold 24 is 0.8358256455775362\nOur F1 score for threshold 25 is 0.8350281588717811\nOur F1 score for threshold 26 is 0.8210681058884545\nOur F1 score for threshold 27 is 0.7754930829460942\nOur F1 score for threshold 28 is 0.6772034216911009\nOur F1 score for threshold 29 is 0.46215415549048783\nOur F1 score for threshold 30 is 0.11412530606190396\nOur F1 score for threshold 31 is 0.09992188838949856\nOur F1 score for threshold 32 is 0.09992188838949856\nOur F1 score for threshold 33 is 0.09992188838949856\nOur F1 score for threshold 34 is 0.09992188838949856\nOur best score : 0.8358256455775362 , threshold : 24\n총 실행시간 : 0:01:04\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                             matches  \\\n0   train_129225211 train_2278313361   \n1  train_3386243561 train_3423213080   \n2  train_2288590299 train_3803689425   \n3  train_2406599165 train_3342059966   \n4   train_3369186413 train_921438619   \n\n                                        pred_matches        f1  \n0  train_129225211 train_2278313361 train_4025803...  0.039216  \n1  train_3386243561 train_3423213080 train_380550...  0.039216  \n2  train_2288590299 train_3803689425 train_295625...  0.039216  \n3  train_2406599165 train_1744956981 train_352677...  0.039216  \n4  train_3369186413 train_921438619 train_2579931...  0.039216  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>matches</th>\n      <th>pred_matches</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>train_129225211 train_2278313361</td>\n      <td>train_129225211 train_2278313361 train_4025803...</td>\n      <td>0.039216</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>train_3386243561 train_3423213080</td>\n      <td>train_3386243561 train_3423213080 train_380550...</td>\n      <td>0.039216</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>train_2288590299 train_3803689425</td>\n      <td>train_2288590299 train_3803689425 train_295625...</td>\n      <td>0.039216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>train_2406599165 train_3342059966</td>\n      <td>train_2406599165 train_1744956981 train_352677...</td>\n      <td>0.039216</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>train_3369186413 train_921438619</td>\n      <td>train_3369186413 train_921438619 train_2579931...</td>\n      <td>0.039216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 해시값으로 동일상품 분류하기","metadata":{}},{"cell_type":"code","source":"# 해시값으로 동일상품 분류\ntmp = df.groupby('image_phash').posting_id.unique().to_dict()\ndf['phash_pred'] = df.image_phash.map(tmp)\n\ndf.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                             matches  \\\n0   train_129225211 train_2278313361   \n1  train_3386243561 train_3423213080   \n2  train_2288590299 train_3803689425   \n3  train_2406599165 train_3342059966   \n4   train_3369186413 train_921438619   \n\n                                        pred_matches        f1  \\\n0  train_129225211 train_2278313361 train_4025803...  0.039216   \n1  train_3386243561 train_3423213080 train_380550...  0.039216   \n2  train_2288590299 train_3803689425 train_295625...  0.039216   \n3  train_2406599165 train_1744956981 train_352677...  0.039216   \n4  train_3369186413 train_921438619 train_2579931...  0.039216   \n\n           phash_pred  \n0   [train_129225211]  \n1  [train_3386243561]  \n2  [train_2288590299]  \n3  [train_2406599165]  \n4  [train_3369186413]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>matches</th>\n      <th>pred_matches</th>\n      <th>f1</th>\n      <th>phash_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>train_129225211 train_2278313361</td>\n      <td>train_129225211 train_2278313361 train_4025803...</td>\n      <td>0.039216</td>\n      <td>[train_129225211]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>train_3386243561 train_3423213080</td>\n      <td>train_3386243561 train_3423213080 train_380550...</td>\n      <td>0.039216</td>\n      <td>[train_3386243561]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>train_2288590299 train_3803689425</td>\n      <td>train_2288590299 train_3803689425 train_295625...</td>\n      <td>0.039216</td>\n      <td>[train_2288590299]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>train_2406599165 train_3342059966</td>\n      <td>train_2406599165 train_1744956981 train_352677...</td>\n      <td>0.039216</td>\n      <td>[train_2406599165]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>train_3369186413 train_921438619</td>\n      <td>train_3369186413 train_921438619 train_2579931...</td>\n      <td>0.039216</td>\n      <td>[train_3369186413]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 예측값 결합","metadata":{}},{"cell_type":"code","source":"if GET_CV:\n    df['image_pred'] = image_predictions\n    df['text_pred'] = text_predictions\n    df['pred_matches'] = df.apply(combine_preds, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df.f1.mean()\n    print('Final F1 CV Score :', score)\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_pred'] = image_predictions\n    df['text_pred'] = text_predictions\n    df['matches'] = df.apply(combine_preds, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Final F1 CV Score : 0.8976507157068873\n","output_type":"stream"}]}]}