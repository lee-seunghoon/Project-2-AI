{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_ImageModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fe5Cw8QYCSySEYjexaPatDcg50EFk6fy",
      "authorship_tag": "ABX9TyO0tEVhkjImcp2R07csDn5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-seunghoon/Project-2-AI/blob/master/Colab_ImageModel0412.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UAEtXnNdTZx",
        "outputId": "ef1bc401-bace-45cc-b73e-08f80c309b1b"
      },
      "source": [
        "!pip install imagehash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting imagehash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB)\n",
            "\r\u001b[K     |█                               | 10kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 14.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 18.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 61kB 23.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 71kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 81kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 92kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 112kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 122kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 133kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 143kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 153kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 163kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 174kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 184kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 194kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 204kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 215kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 225kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 235kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 245kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 256kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 266kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 276kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 286kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imagehash) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imagehash) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imagehash) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imagehash) (1.15.0)\n",
            "Installing collected packages: imagehash\n",
            "Successfully installed imagehash-4.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZeNPouQfm3W",
        "outputId": "f3eeff9f-d794-45b1-ba38-7ea002874ae8"
      },
      "source": [
        "!pip install django"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting django\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/9b/fe94c509e514f6c227308e81076506eb9d67f2bfb8061ce5cdfbde0432e3/Django-3.2-py3-none-any.whl (7.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from django) (2018.9)\n",
            "Collecting asgiref<4,>=3.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/17/8b/05e225d11154b8f5358e6a6d277679c9741ec0339d1e451c9cef687a9170/asgiref-3.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from django) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from asgiref<4,>=3.3.2->django) (3.7.4.3)\n",
            "Installing collected packages: asgiref, django\n",
            "Successfully installed asgiref-3.3.4 django-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NZwqonNf74RC",
        "outputId": "a677d4bd-0129-4ae9-fc71-a73f9ea8d713"
      },
      "source": [
        "!pip install tensorflow-data-validation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-data-validation\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/f9/77d1ac1ed0e1a6729c09e4d36cc5a3b176eb01d77d8e71c32631327c60c1/tensorflow_data_validation-0.29.0-cp37-cp37m-manylinux2010_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.15.0)\n",
            "Collecting joblib<0.15,>=0.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.1.5)\n",
            "Requirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (0.12.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (3.12.4)\n",
            "Collecting pyarrow<3,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/8d/c002e27767595f22aa09ed0d364327922f673d12b36526c967a2bf6b2ed7/pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 200kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<0.30,>=0.29 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (0.29.0)\n",
            "Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (1.19.5)\n",
            "Collecting apache-beam[gcp]<3,>=2.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/e7/d6e5a3786d9a037a38af966bf154bcd6cb3cbea2edffda00cf6c417cc9a2/apache_beam-2.28.0-cp37-cp37m-manylinux2010_x86_64.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 32.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation) (2.4.1)\n",
            "Collecting tfx-bsl<0.30,>=0.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/e9/5ea45abd976774f63ffea7ab9e2c02470efd8431f39aa618db1021642435/tfx_bsl-0.29.0-cp37-cp37m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.0->tensorflow-data-validation) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.0->tensorflow-data-validation) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.9.2->tensorflow-data-validation) (54.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<0.30,>=0.29->tensorflow-data-validation) (1.53.0)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.32.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 33.0MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/c7/9ef9fda5e178aa5f5cda00c0d7505749506c540f9caf876d23c6cf915bf9/fastavro-1.3.5-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (4.1.3)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (3.11.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.3.0)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (0.17.4)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (3.7.4.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.7)\n",
            "Requirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.0.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.21.0)\n",
            "Collecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 38.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (4.2.1)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.28.0)\n",
            "Collecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 29.6MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 32.8MB/s \n",
            "\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.8.0)\n",
            "Collecting google-cloud-build<3,>=2.0.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/04/b566b23f3fdea72326efce6aef09e523b6f8ef3058959672f673c41ffaca/google_cloud_build-2.0.0-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 41.0MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.2)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<0.30,>=0.29->tensorflow-data-validation) (1.12.8)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/a2/16169df906a0a342007ced647199e16163841fc4be7106a493ee42ef6a27/tensorflow_serving_api-2.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (0.6.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (0.4.8)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 36.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (2.4.7)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2,>=0.28.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (1.26.2)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (0.4.1)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/78/20/c862d765287e9e8b29f826749ebae8775bdca50b2cb2ca079346d5fbfd76/fasteners-0.16-py2.py3-none-any.whl\n",
            "Collecting proto-plus>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/7f/4aa1419b0ecb8a31a79fef7a79b49e6a07b977baa6c94612aeeda0228d17/libcst-0.3.18-py3-none-any.whl (512kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (0.4.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.30,>=0.29->tensorflow-data-validation) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<0.30,>=0.29->tensorflow-data-validation) (3.0.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2,>=0.28.1; extra == \"gcp\"->apache-beam[gcp]<3,>=2.28->tensorflow-data-validation) (20.9)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 34.7MB/s \n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1c/66402db44184904a2f14722d317a4da0b5c8c78acfc3faf74362566635c5/typing_inspect-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (3.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2->tensorflow-data-validation) (3.1.0)\n",
            "Building wheels for collected packages: dill, future, avro-python3, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78532 sha256=9be166aa13408e9a7e357a46ece05ecf51910065684dbb4d258ed1c2a327d3e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=d9327ea2b61d164bd4cbfdddf0940176edf6eb14cecf5a9b08b9ac5978c7c880\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=7bc3f61eaa161ec487883d6bd89c852c6c8e3720c26bf427bd32746a6d93d50d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=e3c0b09b1339e4e5042f60091a5c1b41c3139d7a48f95b3d1017240e845497bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18500 sha256=5fdaa46d82f91e254ef08628579cd5be6d6be7910eb8c2dc0c16ccd491a816d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built dill future avro-python3 google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: joblib, pyarrow, dill, requests, hdfs, future, fastavro, pbr, mock, avro-python3, grpc-google-iam-v1, google-cloud-spanner, google-cloud-vision, fasteners, google-apitools, google-cloud-dlp, google-cloud-pubsub, grpcio-gcp, proto-plus, pyyaml, mypy-extensions, typing-inspect, libcst, google-cloud-build, google-cloud-language, google-cloud-bigtable, google-cloud-videointelligence, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-data-validation\n",
            "  Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "Successfully installed apache-beam-2.28.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.3.5 fasteners-0.16 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-build-2.0.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 joblib-0.14.1 libcst-0.3.18 mock-2.0.0 mypy-extensions-0.4.3 pbr-5.5.1 proto-plus-1.18.1 pyarrow-2.0.0 pyyaml-5.4.1 requests-2.25.1 tensorflow-data-validation-0.29.0 tensorflow-serving-api-2.4.1 tfx-bsl-0.29.0 typing-inspect-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWbvmEm778US",
        "outputId": "06f6c5ca-a7a7-40f2-9436-f2ea2adc01c5"
      },
      "source": [
        "# 런타임 다시 시작 후\n",
        "# tensorflow-data-validation 확인\n",
        "import sys, os\n",
        "import tempfile, urllib, zipfile\n",
        "import tensorflow_data_validation as tfdv\n",
        "print('TFDV version: {}'.format(tfdv.version.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFDV version: 0.29.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "0vZGHJho_HUF",
        "outputId": "e4ff5998-c484-476f-9f15-bbc429a96042"
      },
      "source": [
        "# 데이터 준비\n",
        "BASE_DIR = './shopee-product-matching/'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "# OUTPUT_DIR = os.path.join(BASE_DIR, 'shopee_output')\n",
        "TRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\n",
        "EVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\n",
        "# SERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n",
        "\n",
        "## Statics 생성 및 시각화\n",
        "# csv, dataframe, tfrecord 를 통해 Statics 생성\n",
        "[methods for methods in dir(tfdv) if \"generate\" in methods]\n",
        "# 'generate_statistics_from_csv','generate_statistics_from_dataframe','generate_statistics_from_tfrecord']\n",
        "\n",
        "# Statisitcs 생성\n",
        "train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n",
        "# tfdv.generate_statistics_from_csv(data_location, column_names=None)\n",
        "\n",
        "# 시각화\n",
        "tfdv.visualize_statistics(train_stats)\n",
        "#tfdv.visualize_statistics(lhs_statistics, rhs_statistics=None,\n",
        "#                         lhs_name='lhs_statistics', rhs_name='rhs_statistics')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cea8ed0300a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Statisitcs 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_statistics_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# tfdv.generate_statistics_from_csv(data_location, column_names=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py\u001b[0m in \u001b[0;36mgenerate_statistics_from_csv\u001b[0;34m(data_location, column_names, delimiter, output_path, stats_options, pipeline_options, compression_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mskip_header_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_csv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     _ = (\n\u001b[1;32m    178\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_data_validation/utils/stats_gen_lib.py\u001b[0m in \u001b[0;36mget_csv_header\u001b[0;34m(data_location, delimiter)\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatched_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     raise ValueError(\n\u001b[0;32m--> 305\u001b[0;31m         'No file found in the input data location: %s' % data_location)\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;31m# Read the header line in the first file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No file found in the input data location: ./shopee-product-matching/data/train/data.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v0igqz8B22r"
      },
      "source": [
        "## 라이브러리\n",
        "\n",
        "# 파일 처리\n",
        "import os\n",
        "# Data 처리\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Text Color\n",
        "from termcolor import colored\n",
        "# 이미지 및 그래프 출력\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# 경고메시지 지우기\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "# warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# 해쉬(phash) 값 처리\n",
        "import imagehash\n",
        "# 상태바 상태\n",
        "import tqdm\n",
        "from tqdm.auto import tqdm as tqdmp\n",
        "tqdmp.pandas()\n",
        "\n",
        "# \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "# \n",
        "import csv\n",
        "import django\n",
        "import sys\n",
        "\n",
        "#\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "#\n",
        "import skimage.io as io\n",
        "from PIL import Image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTno-wWRMRHx",
        "outputId": "29a7f294-7ae4-439c-aad4-e62c078dba1a"
      },
      "source": [
        "## Work directory\n",
        "WORK_DIR = '/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching'\n",
        "os.listdir(WORK_DIR)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_images',\n",
              " 'test_images',\n",
              " 'sample_submission.csv',\n",
              " 'test.csv',\n",
              " 'train.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ckOkZKOMuVP",
        "outputId": "9144e031-84ba-4898-fe64-9c408b201751"
      },
      "source": [
        "## 데이터 요약\n",
        "train = pd.read_csv('/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching/test.csv')\n",
        "ss = pd.read_csv('/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching/sample_submission.csv', index_col = 0)\n",
        "\n",
        "print('-'*40, 'Train head', '-'*40)\n",
        "print(train.head())\n",
        "print('-'*40, 'Test head', '-'*40)\n",
        "print(test.head())\n",
        "print('-'*30, 'Sample submission head', '-'*30)\n",
        "print(ss.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------- Train head ----------------------------------------\n",
            "         posting_id  ... label_group\n",
            "0   train_129225211  ...   249114794\n",
            "1  train_3386243561  ...  2937985045\n",
            "2  train_2288590299  ...  2395904891\n",
            "3  train_2406599165  ...  4093212188\n",
            "4  train_3369186413  ...  3648931069\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "---------------------------------------- Test head ----------------------------------------\n",
            "        posting_id  ...                                              title\n",
            "0  test_2255846744  ...  Edufuntoys - CHARACTER PHONE ada lampu dan mus...\n",
            "1  test_3588702337  ...  (Beli 1 Free Spatula) Masker Komedo | Blackhea...\n",
            "2  test_4015706929  ...   READY Lemonilo Mie instant sehat kuah dan goreng\n",
            "\n",
            "[3 rows x 4 columns]\n",
            "------------------------------ Sample submission head ------------------------------\n",
            "                         matches\n",
            "posting_id                      \n",
            "test_2255846744  test_2255846744\n",
            "test_3588702337  test_3588702337\n",
            "test_4015706929  test_4015706929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "l6tF2pMdNmYZ",
        "outputId": "10fcd2f0-870c-429b-9abb-a2b01a19413d"
      },
      "source": [
        "print('Train images: %d' %len(os.listdir(os.path.join(WORK_DIR, \"train_images\"))))\n",
        "print('Test images: %d' %len(os.listdir(os.path.join(WORK_DIR, \"test_images\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-89eebf183f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train images: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test images: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching/train_images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmdQnsclIkdc",
        "outputId": "8fc84663-c172-4510-fa5a-4dc59257d352"
      },
      "source": [
        "print('TF',tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "bI_mw-ICCV_T",
        "outputId": "a68ef896-f9ef-4305-cd7c-9ccbe85e49a8"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Machine Learning Colab/Shopee/Shopee_data/train.csv')\n",
        "print('train shape =', train.shape)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape = (34250, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>posting_id</th>\n",
              "      <th>image</th>\n",
              "      <th>image_phash</th>\n",
              "      <th>title</th>\n",
              "      <th>label_group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_129225211</td>\n",
              "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
              "      <td>94974f937d4c2433</td>\n",
              "      <td>Paper Bag Victoria Secret</td>\n",
              "      <td>249114794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_3386243561</td>\n",
              "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
              "      <td>af3f9460c2838f0f</td>\n",
              "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
              "      <td>2937985045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2288590299</td>\n",
              "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
              "      <td>b94cb00ed3e50f78</td>\n",
              "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
              "      <td>2395904891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_2406599165</td>\n",
              "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
              "      <td>8514fc58eafea283</td>\n",
              "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
              "      <td>4093212188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_3369186413</td>\n",
              "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
              "      <td>a6f319f924ad708c</td>\n",
              "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
              "      <td>3648931069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         posting_id  ... label_group\n",
              "0   train_129225211  ...   249114794\n",
              "1  train_3386243561  ...  2937985045\n",
              "2  train_2288590299  ...  2395904891\n",
              "3  train_2406599165  ...  4093212188\n",
              "4  train_3369186413  ...  3648931069\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pRpbjL3JPtD"
      },
      "source": [
        "# Display Random Items from Train Data\n",
        "# 랜덤으로 데이터 출력\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Machine Learning Colab/Shopee/shopee-product-matching/train_images'\n",
        "\n",
        "def displayIMG(df=None, random=False, rows=4, columns=6, path=base_dir):\n",
        "    for r in range(rows):\n",
        "        plt.figure(figsize=(20,5))\n",
        "        for c in range(columns):\n",
        "\n",
        "            if random : \n",
        "                row = np.random.randint(0, len(df))\n",
        "            else: \n",
        "                row = columns*r + c\n",
        "                \n",
        "            name = df.iloc[row, 1]\n",
        "            title =train.iloc[row,3]\n",
        "            \n",
        "            title_with_return = \"\"\n",
        "            for idx,word in enumerate(title):\n",
        "                title_with_return += word\n",
        "                if (idx!=0)&(idx%20==0): title_with_return += '\\n'\n",
        "                    \n",
        "            image = cv2.imread(path + name)\n",
        "            plt.subplot(1,columns, c+1)\n",
        "            plt.title(title_with_return)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(image)\n",
        "        plt.show()\n",
        "        \n",
        "# displayIMG(df=train, random=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "pg5mlN5UK5Og",
        "outputId": "0f9c8572-8a44-4fcd-c708-351598b5e1c0"
      },
      "source": [
        "# Train Image\n",
        "train_images = WORK_DIR + \"/train_images/\" + train['image']\n",
        "train['path'] = train_images\n",
        "\n",
        "test_images = WORK_DIR + \"/test_images/\" + test['image']\n",
        "test['path'] = test_images\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>posting_id</th>\n",
              "      <th>image</th>\n",
              "      <th>image_phash</th>\n",
              "      <th>title</th>\n",
              "      <th>label_group</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_129225211</td>\n",
              "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
              "      <td>94974f937d4c2433</td>\n",
              "      <td>Paper Bag Victoria Secret</td>\n",
              "      <td>249114794</td>\n",
              "      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_3386243561</td>\n",
              "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
              "      <td>af3f9460c2838f0f</td>\n",
              "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
              "      <td>2937985045</td>\n",
              "      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2288590299</td>\n",
              "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
              "      <td>b94cb00ed3e50f78</td>\n",
              "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
              "      <td>2395904891</td>\n",
              "      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_2406599165</td>\n",
              "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
              "      <td>8514fc58eafea283</td>\n",
              "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
              "      <td>4093212188</td>\n",
              "      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_3369186413</td>\n",
              "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
              "      <td>a6f319f924ad708c</td>\n",
              "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
              "      <td>3648931069</td>\n",
              "      <td>/content/drive/MyDrive/Machine Learning Colab/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         posting_id  ...                                               path\n",
              "0   train_129225211  ...  /content/drive/MyDrive/Machine Learning Colab/...\n",
              "1  train_3386243561  ...  /content/drive/MyDrive/Machine Learning Colab/...\n",
              "2  train_2288590299  ...  /content/drive/MyDrive/Machine Learning Colab/...\n",
              "3  train_2406599165  ...  /content/drive/MyDrive/Machine Learning Colab/...\n",
              "4  train_3369186413  ...  /content/drive/MyDrive/Machine Learning Colab/...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iitbaQkRUiiE",
        "outputId": "179143ad-07b7-443d-ef90-c44ceec2f99e"
      },
      "source": [
        "print('label_group unique values: {}'.format(train['label_group'].nunique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label_group unique values: 11014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "m8Ph4tTxU8kp",
        "outputId": "8beee551-59b7-4176-b9cc-dada47b6bc56"
      },
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize = (10, 6))\n",
        "plt.title('Distribution of title length', fontsize = '15')\n",
        "sns.kdeplot(train['title'].apply(lambda x: len(x)), fill = True,\n",
        "            color = 'b',\n",
        "            edgecolor = 'black', alpha = 0.9)\n",
        "plt.xlabel('Title length')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-16a1faecd29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distribution of title length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m sns.kdeplot(train['title'].apply(lambda x: len(x)), fill = True,\n\u001b[1;32m      5\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "6467893c3cf344708493a22ff2d51277"
          ]
        },
        "id": "SPqI2MXyX9IJ",
        "outputId": "9c411a22-931a-4805-c472-320ad2f76e5d"
      },
      "source": [
        "## image shapes distribution\n",
        "## 이미지 모양 출력\n",
        "\n",
        "# Shape columns\n",
        "train['img_shape'] = train['path'].progress_apply(lambda x: np.shape(io.imread(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6467893c3cf344708493a22ff2d51277",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=34250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQv2XA_Pa2JK"
      },
      "source": [
        "shapes = pd.DataFrame().from_records(train['img_shape'])\n",
        "shapes.columns = ['Width', 'Height', 'Colors']\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "sns.jointplot(x = shapes.iloc[:, 0].astype('float32'),\n",
        "              y = shapes.iloc[:, 1].astype('float32'),\n",
        "              height = 8, color = 'r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRWHHf8UeCqR"
      },
      "source": [
        "## Work with image PHASH\n",
        "\n",
        "imagehas.hex_to_hash(train['image_phash'][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA2RV8WBeNRS"
      },
      "source": [
        "def match_matrix(phase_array):\n",
        "\n",
        "\n",
        "    phashs = phash_array.apply(lambda x: imagehash.hex_to_hash(x))\n",
        "    phash_matrix = pd.DataFrame()\n",
        "    pbar = tqdm.tqdm(total = len(phash_array), desc = 'Progress:',\n",
        "                     position = 0, leave = True)\n",
        "    for idx, i in enumerate(phash_array):\n",
        "        pbar.update(1)\n",
        "        phash_matrix = pd.concat([phash_matrix, phashs - imagehash.hex_to_hash(i)],\n",
        "                                 axis = 1)\n",
        "      pbar.close()\n",
        "      phash_matrix.columns = range(len(phash_array))\n",
        "      return phash_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfLs-zKYfveA"
      },
      "source": [
        "train_part = train.iloc[:1000, :]\n",
        "matches = match_matrix(train_part['image_phash'])\n",
        "matches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RwCYXjuf8bY"
      },
      "source": [
        "test_match = match_matrix(test['image_phash'][:3])\n",
        "test_match"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulnva5ZbgUTO"
      },
      "source": [
        "match = []\n",
        "for i in range(len(matches)):\n",
        "    match.append(matches.iloc[i, :][(matches.iloc[i, :] == 0)].index.values)\n",
        "match = pd.Series(match)\n",
        "\n",
        "match[match.apply(lambda x: len(x) > 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aZZbmhsgqn3"
      },
      "source": [
        "def image_viz(image_path)\n",
        "\n",
        "  img = cv2.imread(image_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th8AuEgQhGlp"
      },
      "source": [
        "train_part.loc[[11,12],['posting_id', 'image_phash', 'title', 'label_group']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbwgANHDhPx4"
      },
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "for idx, i in enumerate([train_part.loc[11, 'path'],\n",
        "                         train_part.log[12, 'path']]):\n",
        "    plt.subplot(1, 2, idx + 1)\n",
        "    image_viz(i)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y26Rcrfph1Or"
      },
      "source": [
        "train_part.loc[[889,890,891], ['posting_id', 'image_phash', 'title', 'label_group']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFYK33m7h_jv"
      },
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "for idx, i in enumerate([train_part.loc[889, 'path'],\n",
        "                         train_part.loc[890, 'path'],\n",
        "                         train_part.loc[891, 'path']]):\n",
        "    plt.subplot(1, 3, idx + 1)\n",
        "    image_viz(i)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHv1aXH4ihe6"
      },
      "source": [
        "train_part.loc[[997,520],['posting_id', 'image_phash', 'title', 'label_group']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_nblZtisj5"
      },
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "for idx, i in enumerate([train_part.loc[997, 'path'],\n",
        "                         train_part.loc[520, 'path']]):\n",
        "    plt.subplot(1, 2, idx + 1)\n",
        "    image_viz(i)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hobJVaMJjEJ5"
      },
      "source": [
        "match = []\n",
        "for i in range(len(matches)):\n",
        "  match.append(matches.iloc[i, :][(matches.iloc[i, :] > 0) &\n",
        "                                  (matches.iloc[i, :] <= 5).index.values])\n",
        "match = pd.Series(match)\n",
        "\n",
        "match[match.apply(lambda x: len(x) >= 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Dnf_acjwxz"
      },
      "source": [
        "### Baseline 예측\n",
        "\n",
        "# Work functions\n",
        "def phash_match(phash_array, element)\n",
        "    \"\"\"\n",
        "    A function that calculates phash diffs.\n",
        "    Takes phashs array and element as input.\n",
        "    Output - phash diff\n",
        "    \"\"\"\n",
        "    phash_diff = phash_array - phash_array[element]\n",
        "    return phash_diff\n",
        "\n",
        "def add_match(phash, i, dataset = train, threshold = 5):\n",
        "\n",
        "    diffs = phash_match(phash, i)\n",
        "    matches = [x for x in diffs[diffs <= threshold].index.drop(i).values]\n",
        "    str_matches = ''\n",
        "    str_matches = str_matches + dataset.iloc[i, 0] + ' '\n",
        "    for j in matches:\n",
        "        str_matches = str_matches + dataset.iloc[j, 0] + ' '\n",
        "    str_matches = str_matches[:-1]\n",
        "    retrun str_matches\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pml6XPnhlKOn"
      },
      "source": [
        "phashs = train['image_phash'][:1000].apply(lambda x: imagehash.hex_to_hash(x))\n",
        "str_matches = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(phashs)), desc = 'Progress:', position = 0, leave = True):\n",
        "    str_matches.append(add_match(phashs, i))\n",
        "\n",
        "str_matches[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmEjprGQlme4"
      },
      "source": [
        "### Test images\n",
        "plt.figure(figsize = (15, 10))\n",
        "for idx, i in enumerate(test['path']):\n",
        "    plt.subplot(1, 3, idx +1)\n",
        "    image_viz(i)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXGFTxDeocoH"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vLQSq8iodsQ"
      },
      "source": [
        "test_phashs = test['image_phash'].apply(lambda x: imagehash.hex_to_hash(x))\n",
        "test_matches = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(test_phashs)), desc = 'Progress:',\n",
        "                   position = 0, leave = True):\n",
        "    test_matches.append(add_match(test_phashs, i, test, threshold = 7))\n",
        "\n",
        "test_matches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQc-v-WPpl8M"
      },
      "source": [
        "ss['matches'] = test_matches\n",
        "ss.to_csv(\"submission.csv\")\n",
        "ss\n",
        "## 이 방법으로는 쉽고 간단한지만 시간이 매우 오래 걸림 ##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPqZyo2fp8kJ"
      },
      "source": [
        "def simple_match(dataset, element):\n",
        "\n",
        "    matches = dataset[dataset['image_phash'] == \n",
        "                      dataset['image_phash'][element]]['posting_id'].drop(element).\n",
        "values\n",
        "    str_matches = ''\n",
        "    str_matches = str_matches + dataset.iloc[element, 0] + ' '\n",
        "    for j in matches:\n",
        "        str_matches = str_matches + j + ' '\n",
        "    str_matches = str_matches[:-1]\n",
        "    return str_matches\n",
        "                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEyhx-9WrV-R"
      },
      "source": [
        "train_for_s = train[['posting_id', 'image_phash']]\n",
        "str_matches = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(train_for_s)), desc = 'Progress:',\n",
        "                  position = 0, leave = True):\n",
        "    str_matches.append(simple_match(train_for_s, i))\n",
        "\n",
        "str_matches[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rAuJd9tsI3Y"
      },
      "source": [
        "test_for_s = test.loc[:2, ['posting_id', 'image_phash']]\n",
        "test_matches = []\n",
        "\n",
        "for i in tqdm.tqdm(range(len(test_for_s)), desc = 'Progress:', \n",
        "                   position = 0, leave = True):\n",
        "    test_matches.append(simple_match(test_for_s, i))\n",
        "\n",
        "test_matches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-FvchtaLjJj"
      },
      "source": [
        "# 학습이 종료됨. 모델 저장\n",
        "# model.save('/content/drive/MyDrive/Machine Learning Colab/CAT_DOG/cat_dog_small_cnn_tf2.4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta7uuaixsfPT"
      },
      "source": [
        "## ss['matches'] = test_matches\n",
        "# ss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvMy8ph7soHn"
      },
      "source": [
        "# ss.to_csv(\"submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTq7hBp2E3qy",
        "outputId": "f2ab4e30-80b1-4530-9d3a-45a3f2576960"
      },
      "source": [
        "# # 경로 지정\n",
        "# # 기본 경로\n",
        "# base_dir = '/shopee_filtered'\n",
        "\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# train_baseline_dir = os.path.join(train_dir, 'baseline')\n",
        "# print(train_baseline_dir)\n",
        "\n",
        "# # 테스트에 사용되는 고양이/개 이미지 경로\n",
        "# validation_baseline_dir = os.path.join(validation_dir, 'baseline')\n",
        "\n",
        "# print(validation_baseline_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/shopee_filtered/train/baseline\n",
            "/shopee_filtered/validation/baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "L2kpNJRSEoyL",
        "outputId": "7694fb25-e300-488d-b537-82870cf919b8"
      },
      "source": [
        "# ## dataset 확인\n",
        "# # 파일 이름과 개수\n",
        "# train_baseline_fnames = os.listdir( train_baseline_dir )\n",
        "\n",
        "# print(train_baseline_fnames[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-161ada454203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## dataset 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 파일 이름과 개수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_baseline_fnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_baseline_dir\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/shopee_filtered/train/baseline'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "5FhIxoFQEBt-",
        "outputId": "6f7ef124-5aa2-4a7f-b984-ded7308db257"
      },
      "source": [
        "# # 이미지 확인\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# import matplotlib.image as mpimg\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# nrows, ncols = 4, 4\n",
        "# pic_index = 0\n",
        "\n",
        "# fig = plt.gcf()\n",
        "# fig.set_size_inches(ncols*3, nrows*3)\n",
        "\n",
        "# pic_index+=8\n",
        "\n",
        "# next_baseline_pix = [os.path.join(train_baseline_dir, fname)\n",
        "#                 for fname in train_baseline_fnames[ pic_index-8:pic_index]]\n",
        "\n",
        "# for i, img_path in enumerate(next_baseline_pix+next_dog_pix):\n",
        "#   sp = plt.subplot(nrows, ncols, i + 1)\n",
        "#   sp.axis('Off')\n",
        "\n",
        "#   img = mpimg.imread(img_path)\n",
        "#   plt.imshow(img)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8deafaab6572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m next_baseline_pix = [os.path.join(train_baseline_dir, fname)\n\u001b[0;32m---> 17\u001b[0;31m                 for fname in train_baseline_fnames[ pic_index-8:pic_index]]\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_baseline_pix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnext_dog_pix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_baseline_fnames' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVhHvIN-DWSD",
        "outputId": "b5dce0a4-53ac-44a8-b2ec-48040936e195"
      },
      "source": [
        "# # model 구성\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#   tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "#   tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "#   tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "#   tf.keras.layers.MaxPooling2D(2,2),\n",
        "#   tf.keras.layers.Flatten(),\n",
        "#   tf.keras.layers.Dense(512, activation='relu'),\n",
        "#   tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 148, 148, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 72, 72, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18496)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               9470464   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 9,494,561\n",
            "Trainable params: 9,494,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rzwdA-nDwgX"
      },
      "source": [
        "# 모델 컴파일\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}